{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN 모듈 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 OpenCV DNN 모듈\n",
    "- OpenCV DNN 모듈\n",
    "    - 미리 학습된 딥러닝 모델을 이용하여 실행(forward pass, inference) 하는 기능\n",
    "    - 학습은 지원하지 않음\n",
    "    - OpenCV 3.3 버전부터 기본 기능으로 제공\n",
    "    - OpenCV 4.3 버전부터 GPU(CUDA) 지원 (소스 코드 직접 빌드 필요)\n",
    "    - 참고 : https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV\n",
    "- 지원하는 딥러닝 프레임워크\n",
    "    - Caffe, Tensorflow, Pytorch, Darknet, ONNX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 네트워크 불러오기\n",
    "- **cv2.dnn.readNet(model, config=None, framework=None) -> retval**\n",
    "    - model : 훈련된 가중치를 저장하고 있는 이진 파일 이름\n",
    "    - config : 네트워크 구성을 저장하고 있는 텍스트 파일 이름\n",
    "    - framework : 명시적인 딥러닝 프레임워크 이름\n",
    "    - retval : cv2.dnn_Net 클래스 객체"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 네트워크 입력 블롭(blob) 만들기\n",
    "- **cv2.dnn.blobFromImage(image, scalefactor=None, size=None, mean=None,\n",
    "                          swapRB=None, crop=None, ddepth=None) -> retval**\n",
    "    - image : 입력 영상\n",
    "    - scalefactor : 입력 영상 픽셀 값에 곱할 값. 기본값은 1\n",
    "    - size : 출력 영상의 크기. 기본값은 (0, 0)\n",
    "    - mean : 입력 영상 각 채널에서 뺄 평균 값. 기본값은 (0, 0, 0, 0)\n",
    "    - swapRB : R과 B채널을 서로 바꿀 것인지를 결정하는 플래그. 기본값은 False\n",
    "    - crop : 크롭(crop) 수행 여부. 기본값은 False\n",
    "    - ddepth : 출력 블롭의 깊이. CV_32F 또는 CV_8U. 기본값은 CV_32F\n",
    "    - retval : 영상으로부터 구한 블롭 객체.\n",
    "        - numpy.ndarray.shape = (N,C,H,W), dtype = numpy.float32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 네트워크 입력 설정\n",
    "- **cv2.dnn_Net.setInput(blob, name=None, scalefactor=None, mean=None) -> None**\n",
    "    - blob : 블롭 객체\n",
    "    - name : 입력 레이어 이름\n",
    "    - scalefactor : 추가적으로 픽셀 값에 곱할 값\n",
    "    - mean : 추가적으로 픽셀 값에서 뺄 평균 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 네트워크 순방향추론\n",
    "- **cv2.dnn_Net.forward(outputName=None) -> retval**\n",
    "- **cv2.dnn_Net.forward(outputNames=None, outputBlobs=None) -> outputBlobs**\n",
    "    - outputName : 출력 레이어 이름\n",
    "    - retval : 지정한 레이어의 출력 블롭. 네트워크마다 다르게 결정됨\n",
    "    - outputNames : 출력 레이어 이름 리스트\n",
    "    - outputBlobs : 지정한 레이어의 출력 블롭 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "mnist = datasets.mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "X_train = X_train.reshape((60000, 28, 28, 1))\n",
    "X_test = X_test.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        8256      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              3137000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                10010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,156,098\n",
      "Trainable params: 3,156,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                  padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    layers.Conv2D(64, (2, 2), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1000, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60/60 [==============================] - 21s 346ms/step - loss: 0.4514 - accuracy: 0.8693 - val_loss: 0.1132 - val_accuracy: 0.9661\n",
      "Epoch 2/5\n",
      "60/60 [==============================] - 21s 351ms/step - loss: 0.1014 - accuracy: 0.9696 - val_loss: 0.0617 - val_accuracy: 0.9818\n",
      "Epoch 3/5\n",
      "60/60 [==============================] - 21s 356ms/step - loss: 0.0682 - accuracy: 0.9793 - val_loss: 0.0435 - val_accuracy: 0.9855\n",
      "Epoch 4/5\n",
      "60/60 [==============================] - 23s 382ms/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0413 - val_accuracy: 0.9859\n",
      "Epoch 5/5\n",
      "60/60 [==============================] - 23s 392ms/step - loss: 0.0449 - accuracy: 0.9865 - val_loss: 0.0328 - val_accuracy: 0.9888\n",
      "fit time :  110.05416107177734\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "hist = model.fit(X_train, y_train, epochs=5, verbose=1, batch_size=1000,\n",
    "                 validation_data = (X_test, y_test))\n",
    "print('fit time : ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 test accuracy 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 0.0328 - accuracy: 0.9888\n",
      "Test loss : 0.032845389097929\n",
      "Test accuracy : 0.9887999892234802\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test loss :', score[0])\n",
    "print('Test accuracy :', score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('C:/seonwook/DL/OpenCV/mnist_cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 pb파일 생성을 위한 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = tf.function(lambda x: model(x))\n",
    "full_model = full_model.get_concrete_function(\n",
    "    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frozen ConcreteFunction\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "\n",
    "frozen_func = convert_variables_to_constants_v2(full_model)\n",
    "# frozen_func.graph.as_graph_def() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델의 구조와 웨이트가 모두 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 레이어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [op.name for op in frozen_func.graph.get_operations()]\n",
    "# print('-' * 50)\n",
    "# print('Frozen model layers: ')\n",
    "# for layer in layers:\n",
    "#     print(layer)\n",
    "\n",
    "# print('-' * 50)\n",
    "# print('Frozen model inputs: ')\n",
    "# print(frozen_func.inputs)\n",
    "# print('Frozen model outputs: ')\n",
    "# print(frozen_func.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13 pb로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save frozen graph from frozen ConcreteFunction to hard drive\n",
    "# tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "#                   logdir='./frozen_models',\n",
    "#                   name='frozen_graph.pb',\n",
    "#                   as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.14 opencv에서 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (99.93%)\n",
      "7 (82.44%)\n",
      "7 (63.07%)\n",
      "5 (100.00%)\n",
      "4 (99.88%)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "oldx, oldy = -1, -1\n",
    "\n",
    "def on_mouse(event, x, y, flags, _):\n",
    "    global oldx, oldy\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "    \n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        oldx, oldy = -1, -1\n",
    "    \n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x, y), (255, 255, 255), 40, cv2.LINE_AA)\n",
    "            oldx, oldy = x, y\n",
    "            cv2.imshow('img', img)\n",
    "\n",
    "net = cv2.dnn.readNetFromTensorflow('./frozen_models/frozen_graph.pb')\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load failed')\n",
    "    sys.exit()\n",
    "\n",
    "img = np.zeros((400, 400), np.uint8)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.setMouseCallback('img', on_mouse)\n",
    "\n",
    "while True:\n",
    "    c = cv2.waitKey()\n",
    "\n",
    "    if c == 27:\n",
    "        break\n",
    "    elif c == ord(' '):\n",
    "        blob = cv2.dnn.blobFromImage(img, 1/255., (28, 28))\n",
    "        net.setInput(blob)\n",
    "        prob = net.forward()\n",
    "\n",
    "        _, maxVal, _, maxLoc = cv2.minMaxLoc(prob)\n",
    "        digit = maxLoc[0]\n",
    "\n",
    "        print(f'{digit} ({maxVal * 100:4.2f}%)')\n",
    "\n",
    "        img.fill(0)\n",
    "        cv2.imshow('img', img)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.15 모멘트\n",
    "- 모멘트를 이용해서 그림의 중심을 찾을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m00': 27864765.0, 'm10': 11708252630.0, 'm01': 5540303468.0, 'm20': 5482973170932.0, 'm11': 2330559803482.0, 'm02': 1417073617236.0, 'm30': 2714153882789594.0, 'm21': 1089720437669904.0, 'm12': 607718273485288.0, 'm03': 408657928000928.0, 'mu20': 563384592025.919, 'mu11': 2627999635.489337, 'mu02': 315504574126.0788, 'mu30': -63136121678841.586, 'mu21': -2658193233792.011, 'mu12': 11245425539295.326, 'mu03': 1441141913370.1262, 'nu20': 0.0007255948570512897, 'nu11': 3.3846559647410453e-06, 'nu02': 0.00040634497215981397, 'nu30': -1.5404210288365443e-05, 'nu21': -6.485569032689687e-07, 'nu12': 2.7437051117999384e-06, 'nu03': 3.51615723275736e-07}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "src = cv2.imread(\"digits_test.jpg\")\n",
    "gray = cv2.cvtColor(src, cv2.COLOR_RGB2GRAY)\n",
    "M = cv2.moments(gray)\n",
    "print(M)\n",
    "cX = int(M['m10'] / M['m00'])\n",
    "cY = int(M['m01'] / M['m00'])\n",
    "cv2.circle(src, (cX, cY), 5, (255, 0, 0), -1)\n",
    " \n",
    "cv2.imshow(\"src\", src)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모멘트 (이미지의 경우)\n",
    "    - 이미지의 픽셀 값이 0이 아닌 값은 모두 1의 값으로 변경해 모멘트를 계산\n",
    "    - 모멘트 함수를 통해 면적, 평균, 분산 등을 간단하게 계산할 수 있음\n",
    "    - 중심점 구하는 공식\n",
    "        - $ \\overline{x} = \\frac{m_{10}}{m_{00}}, \\overline{y} = \\frac{m_{01}}{m_{00}}$\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m00': 8019652.0, 'm10': 1287200513.0, 'm01': 1520047179.0, 'm20': 247868201363.0, 'm11': 248935872883.0, 'm02': 324460355481.0, 'm30': 54061671719993.0, 'm21': 48993378080417.0, 'm12': 54018531456523.0, 'm03': 75334409790903.0, 'mu20': 41265076854.921196, 'mu11': 4959512238.797126, 'mu02': 36350169853.76101, 'mu30': 1030825475777.053, 'mu21': 420306126465.50146, 'mu12': 60713686013.18706, 'mu03': 56455793523.58839, 'nu20': 0.0006416107206567965, 'nu11': 7.711305695196683e-05, 'nu02': 0.0005651912089698955, 'nu30': 5.6597381025015525e-06, 'nu21': 2.3076870474882663e-06, 'nu12': 3.333479528269529e-07, 'nu03': 3.099700319994036e-07}\n",
      "4 (99.99%)\n",
      "{'m00': 6114319.0, 'm10': 1268005205.0, 'm01': 1193259224.0, 'm20': 276430879761.0, 'm11': 254906896246.0, 'm02': 262410457866.0, 'm30': 62922858572987.0, 'm21': 57043534970262.0, 'm12': 57489486994654.0, 'm03': 62570928934994.0, 'mu20': 13468283287.526495, 'mu11': 7445338066.542739, 'mu02': 29536187540.705853, 'mu30': 9637823190.765766, 'mu21': 7722492049.818538, 'mu12': 164008381594.70523, 'mu03': -169068074018.79465, 'nu20': 0.0003602600094743008, 'nu11': 0.00019915363414401918, 'nu02': 0.0007900566817676156, 'nu30': 1.0425783965843133e-07, 'nu21': 8.353860845516516e-08, 'nu12': 1.7741723636643096e-06, 'nu03': -1.8289059472786176e-06}\n",
      "5 (81.02%)\n",
      "{'m00': 5740280.0, 'm10': 1316701980.0, 'm01': 1052499387.0, 'm20': 323614935088.0, 'm11': 246373430844.0, 'm02': 233971387675.0, 'm30': 83335943366736.0, 'm21': 61360489267406.0, 'm12': 55853269429720.0, 'm03': 59861198442051.0, 'mu20': 21590625448.76282, 'mu11': 4951753362.816807, 'mu02': 40992132371.186295, 'mu30': -799545554290.4751, 'mu21': -247043417192.98773, 'mu12': 369212175910.1046, 'mu03': 1929681079053.084, 'nu20': 0.0006552379900325669, 'nu11': 0.00015027711579216932, 'nu02': 0.0012440400342169859, 'nu30': -1.0127692818871054e-05, 'nu21': -3.1292523969757675e-06, 'nu12': 4.676741034377694e-06, 'nu03': 2.4442906476267157e-05}\n",
      "7 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "oldx, oldy = -1, -1\n",
    "\n",
    "def on_mouse(event, x, y, flags, _):\n",
    "    global oldx, oldy\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "    \n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        oldx, oldy = -1, -1\n",
    "    \n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x, y), (255, 255, 255), 40, cv2.LINE_AA)\n",
    "            oldx, oldy = x, y\n",
    "            cv2.imshow('img', img)\n",
    "\n",
    "def norm_digit(img):\n",
    "    m = cv2.moments(img)\n",
    "    print(m)\n",
    "    cx = m['m10'] / m['m00']\n",
    "    cy = m['m01'] / m['m00']\n",
    "    h, w = img.shape[:2]\n",
    "    aff = np.array([[1, 0, w/2 - cx], [0, 1, h/2 - cy]], dtype=np.float32)\n",
    "    dst = cv2.warpAffine(img, aff, (0, 0))\n",
    "    return dst\n",
    "\n",
    "net = cv2.dnn.readNet('./frozen_models/frozen_graph.pb')\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load failed')\n",
    "    sys.exit()\n",
    "\n",
    "img = np.zeros((400, 400), np.uint8)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.setMouseCallback('img', on_mouse)\n",
    "\n",
    "while True:\n",
    "    c = cv2.waitKey()\n",
    "\n",
    "    if c == 27:\n",
    "        break\n",
    "    elif c == ord(' '):\n",
    "        blob = cv2.dnn.blobFromImage(norm_digit(img), 1/255., (28, 28))\n",
    "        net.setInput(blob)\n",
    "        prob = net.forward()\n",
    "\n",
    "        _, maxVal, _, maxLoc = cv2.minMaxLoc(prob)\n",
    "        digit = maxLoc[0]\n",
    "\n",
    "        print(f'{digit} ({maxVal * 100:4.2f}%)')\n",
    "\n",
    "        img.fill(0)\n",
    "        cv2.imshow('img', img)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 학습 모델 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 GoogLeNet\n",
    "- 2014년 ILSVRC(ImageNet Large Scale Visual Revognition Competition) 영상 인식 분야 1위\n",
    "    - 1000개의 카테고리, 120만개의 훈련 영상, 15만개의 테스트 영상\n",
    "- 입력 : 224x224, BGR 컬러 영상, 평균 값 = (104, 117, 123)\n",
    "- 출력 : 1x1000 행렬, 1000개 클래스에 대한 확률 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2 모델 받기\n",
    "- Caffe model https://github.com/BVLC/caffe\n",
    "- Model : http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel\n",
    "- 설정파일 : https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/deploy.prototxt\n",
    "- ONNX model : https://github.com/onnx/models\n",
    "- ONNX 모델 파일 : https://github.com/onnx/models/tree/master/vision/classification/inception_and_googlenet/googlenet\n",
    "- GoogLeNet 클래스 이름 파일 : https://github.com/opencv/opencv/blob/4.1.0/samples/data/dnn/classification_classes_ILSVRC2012.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image load failed!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# 입력 영상 불러오기\n",
    "\n",
    "filename = 'Model_S.jpg'\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    filename = sys.argv[1]\n",
    "\n",
    "img = cv2.imread(filename)\n",
    "\n",
    "if img is None:\n",
    "    print('Image load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 네트워크 불러오기\n",
    "\n",
    "# Caffe\n",
    "model = 'googlenet/bvlc_googlenet.caffemodel'\n",
    "config = 'googlenet/deploy.prototxt'\n",
    "\n",
    "# ONNX\n",
    "#model = 'googlenet/inception-v1-9.onnx'\n",
    "#config = ''\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 클래스 이름 불러오기\n",
    "\n",
    "classNames = None\n",
    "with open('googlenet/classification_classes_ILSVRC2012.txt', 'rt') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "# 추론\n",
    "blob = cv2.dnn.blobFromImage(img, 1, (224, 224), (104, 117, 123))\n",
    "net.setInput(blob)\n",
    "prob = net.forward()\n",
    "\n",
    "# 추론 결과 확인 & 화면 출력\n",
    "out = prob.flatten()\n",
    "classId = np.argmax(out)\n",
    "confidence = out[classId]\n",
    "\n",
    "text = f'{classNames[classId]} ({confidence * 100:4.2f}%)'\n",
    "cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv2_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e35e6c96d093a8df781201be47e86c8f46ad7606019972a628a64ac225b9e401"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
